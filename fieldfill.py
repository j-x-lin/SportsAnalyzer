# -*- coding: utf-8 -*-
"""CIFAR-10_Networks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eahFCHg2_6vQkpvdT8jUZTZXdwfFH9b8
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

"""
Training a Classifier
=====================

This is it. You have seen how to define neural networks, compute loss and make
updates to the weights of the network.

Now you might be thinking,

What about data?
----------------

Generally, when you have to deal with image, text, audio or video data,
you can use standard python packages that load data into a numpy array.
Then you can convert this array into a ``torch.*Tensor``.

-  For images, packages such as Pillow, OpenCV are useful
-  For audio, packages such as scipy and librosa
-  For text, either raw Python or Cython based loading, or NLTK and
   SpaCy are useful

Specifically for vision, we have created a package called
``torchvision``, that has data loaders for common datasets such as
Imagenet, CIFAR10, MNIST, etc. and data transformers for images, viz.,
``torchvision.datasets`` and ``torch.utils.data.DataLoader``.

This provides a huge convenience and avoids writing boilerplate code.

For this tutorial, we will use the CIFAR10 dataset.
It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’,
‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of
size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.

.. figure:: /_static/img/cifar10.png
   :alt: cifar10

   cifar10


Training an image classifier
----------------------------

We will do the following steps in order:

1. Load and normalize the CIFAR10 training and test datasets using
   ``torchvision``
2. Define a Convolutional Neural Network
3. Define a loss function
4. Train the network on the training data
5. Test the network on the test data

1. Load and normalize CIFAR10
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Using ``torchvision``, it’s extremely easy to load CIFAR10.

"""

import torch
import torchvision
import torchvision.transforms as transforms

import numpy as np
import matplotlib.pyplot as plt

"""The output of torchvision datasets are PILImage images of range [0, 1].
We transform them to Tensors of normalized range [-1, 1].

<div class="alert alert-info"><h4>Note</h4><p>If running on Windows and you get a BrokenPipeError, try setting
    the num_worker of torch.utils.data.DataLoader() to 0.</p></div>
"""

# Data augmentation and normalization for training
# Just normalization for validation
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

batch_size = 64


fulltrain = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transforms.ToTensor())

fulltrainSize = fulltrain.data.shape[0]

trainset, valset = torch.utils.data.random_split(fulltrain, [45000, 5000])

trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=2)

valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transforms.ToTensor())

testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False, num_workers=2)

class_names = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

dataloaders = {'train': trainloader, 'test': testloader, 'val': valloader}
dataset_sizes = {'train': 45000,
                 'test': 10000,
                 'val': 5000}

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

"""2. Define a Convolutional Neural Network
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Copy the neural network from the Neural Networks section before and modify it to
take 3-channel images (instead of 1-channel images as it was defined).


"""

import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim


class ConvNet(nn.Module):
    def __init__(self, k, m, n):
        super().__init__()
        self.conv1 = nn.Conv2d(3, m, k)
        self.pool = nn.MaxPool2d(n, n)
        self.fc1 = nn.Linear(int(m*( np.trunc((33-k)/n) )**2), 120)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = self.fc1(x)
        return x

class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 8, 5)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(8, 16, 5)
        self.conv3 = nn.Conv2d(16, 64, 5)
        self.fc1 = nn.Linear(64 * 5 * 5, 1200000)
        self.fc2 = nn.Linear(1200000, 840000)
        self.fc3 = nn.Linear(840000, 720 * 720 * 3)

    def forward(self, x):
        x = self.pool1(F.relu(self.conv1(x)))
        x = F.relu(self.conv2(x))
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()
convNet = ConvNet(5, 200, 2)

"""4. Train the network
^^^^^^^^^^^^^^^^^^^^

This is when things start to get interesting.
We simply have to loop over our data iterator, and feed the inputs to the
network and optimize.


"""

# conv_lines_train = {}
# no_lines_train = {}
# one_lines_train = {}
# net_lines_train = {}

# conv_lines_val = {}
# no_lines_val = {}
# one_lines_val = {}
# net_lines_val = {}

def train_model(model):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

    acc_train = 0
    acc_val = 0
    
    epochs = 0

    acc_train_arr = np.zeros(0)
    acc_val_arr = np.zeros(0)

    loss_train_arr = np.zeros(0)
    loss_val_arr = np.zeros(0)
    
    while acc_val < 0.63:
        correct = 0

        running_loss = 0.0
        
        for i, data in enumerate(trainloader, 0):
            # get the inputs; data is a list of [inputs, labels]
            inputs, labels = data

            # zero the parameter gradients
            optimizer.zero_grad()
            
            # forward + backward + optimize
            outputs = model(inputs)

            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            y = model.forward(inputs)
            for i in range(y.shape[0]):
                if torch.argmax(y[i]) == labels[i]:
                    correct += 1
            
            # print statistics
            running_loss += loss.item()

        acc_train = correct / 45000
        
        acc_train_arr = np.append(acc_train_arr, acc_train)
        loss_train_arr = np.append(loss_train_arr, running_loss)

        print('-'*10)
        print('Epoch', epochs)

        print('Train loss:', running_loss)
        
        print('Train accuracy:', acc_train)
        
        correct = 0

        running_loss = 0.0
        
        for i, data in enumerate(valloader, 0):
            # get the inputs; data is a list of [inputs, labels]
            inputs, labels = data

            # forward
            outputs = model(inputs)

            loss = criterion(outputs, labels)

            y = model.forward(inputs)
            for i in range(y.shape[0]):
                if torch.argmax(y[i]) == labels[i]:
                    correct += 1

            # print statistics
            running_loss += loss.item()

        acc_val = correct / 5000
        
        acc_val_arr = np.append(acc_val_arr, acc_val)
        loss_val_arr = np.append(loss_val_arr, running_loss)
        
        print('Validation loss:', running_loss)
        
        print('Validation accuracy:', acc_val)
        
        print('-'*10)
        print()

        epochs += 1

    print('Finished Training')
    return acc_train_arr, acc_val_arr, loss_train_arr, loss_val_arr, epochs

# net = Net()
# convNet = ConvNet(5, 200, 2)
# noHiddenNet = NoHiddenNet()
# oneHiddenNet = OneHiddenNet(1536)
acc_train_arr, acc_val_arr, loss_train_arr, loss_val_arr, epochs = train_model(net)

plt.plot(np.arange(epochs), acc_train_arr, label='train k=5 m=8 n=2')
plt.plot(np.arange(epochs), acc_val_arr, label='val k=5 m=8 n=2')
plt.show()

test_acc, test_loss = test_model(net)
print(test_acc, test_loss)

def test_model(model):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

    acc = 0
    correct = 0

    running_loss = 0.0

    for i, data in enumerate(testloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # forward + backward + optimize
        outputs = model(inputs)

        loss = criterion(outputs, labels)

        y = model.forward(inputs)
        for i in range(y.shape[0]):
            if torch.argmax(y[i]) == labels[i]:
                correct += 1

        # print statistics
        running_loss += loss.item()

    acc = correct / 10000

    print('Test loss:', running_loss)
    print('Test accuracy:', acc)

    return acc, running_loss

# no_lines_train[0.005] = np.append(acc_no_arr, epochs)
# one_lines_train[1536] = np.append(acc_one_arr, epochs)
# conv_lines_train[200] = np.append(acc_conv_arr, epochs)

# no_lines_val[0.005] = np.append(acc_no_val_arr, epochs)
# one_lines_val[1536] = np.append(acc_one_val_arr, epochs)
# conv_lines_val[200] = np.append(acc_conv_val_arr, epochs)

for hyperparam in no_lines_train:
    arr = no_lines_train[hyperparam]
    plt.plot(np.arange(arr[-1]), arr[: arr.size-2], label='lr=' + str(hyperparam) + ' train')

for hyperparam in no_lines_val:
    arr = no_lines_val[hyperparam]
    plt.plot(np.arange(arr[-1]), arr[: -1], label='lr=' + str(hyperparam) + ' val')

plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.title('No Hidden Layers')
plt.legend()
plt.show()

for hyperparam in one_lines_train:
    arr = one_lines_train[hyperparam]
    plt.plot(np.arange(arr[-1]), arr[: -2], label='size=' + str(hyperparam) + ' train')

for hyperparam in one_lines_train:
    arr = one_lines_val[hyperparam]
    plt.plot(np.arange(arr[-1]), arr[: -1], label='size=' + str(hyperparam) + ' val')

plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.title('One Hidden Layer')
plt.legend()
plt.show()

for hyperparam in conv_lines_train:
    arr = conv_lines_train[hyperparam]
    plt.plot(np.arange(arr[-1]), arr[: -2], label='m=' + str(hyperparam) + ' train')

for hyperparam in conv_lines_val:
    arr = conv_lines_val[hyperparam]
    plt.plot(np.arange(arr[-1]), arr[: -1], label='m=' + str(hyperparam) + ' val')

plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.title('Convolutional network')
plt.legend()
plt.show()